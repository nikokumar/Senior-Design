# -*- coding: utf-8 -*-
"""I7_sentiment_analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PUbtkdHpKWTyTVLMlkaIjk4Jw-nbyvSY
"""

!pip install yfinance

import yfinance as yf
import pandas as pd
import numpy as np


ticker = "txn"
stock_data = yf.download(ticker, start="2010-01-04", end="2021-05-12")
stock_data = pd.DataFrame(stock_data)
stock_data.dropna(inplace=True)
stock_data.describe()

y = stock_data['Adj Close']
X = stock_data.drop('Adj Close', axis = 1)

!pip show tweepy

import tweepy #DO NOT RUN DOES NOT WORK
consumer_key= 'g8JVJxnUO0tm5tzAuGE5c8xjs'
consumer_secret= 'WwYA6cqRnfqVdjpVRaxiNRlBY1rdZQEUo9f1pE3C6NPOsl4oMG'
access_token= '1490928344594100225-vOJZ9af3nQ03cX7lTwIvOtZUj88BOh'
access_token_secret= 'H7gNf5ZtKCdzoyB5Je8neOmfsZ5Wyzd07c1KD7Ebs3XWa'

client = tweepy.Client(consumer_key= consumer_key,
                       consumer_secret= consumer_secret,
                       access_token= access_token,
                       access_token_secret= access_token_secret)
query = 'TXN'
tweets = client.search_recent_tweets(query=query, max_results=10)
for tweet in tweets.data:
    print(tweet.text)

!pip install praw

import praw
import spacy
import nltk

def retrive_submission_props(reddit, subreddit_topic):

    for submission in reddit.subreddit(subreddit_topic).hot(limit=1):
        return (submission, submission.title, submission.num_comments, submission.selftext)


reddit = praw.Reddit(
client_id="PolSzQDXMbO5Lcfh72cWhg",
client_secret="bpLb2dFboHpyit5I8PDtOYBXFieyww",
user_agent="I7_Senior_Design",)

submission, submission_title, submission_num_comments, submission_selftext =retrive_submission_props(reddit,"wallstreetbets")

submission

submission_title

submission_num_comments

submission_selftext

def read_comments(submission, limit =0):
    
    all_comments_body =[]
    
    submission.comments.replace_more(limit=limit)
    
    for top_level_comment in submission.comments:
        
        # As the output will have more comments
        if isinstance(top_level_comment, MoreComments):
            continue
            
        all_comments_body.append(top_level_comment.body)
            
        
    return all_comments_body, top_level_comment

import re #doesn't work yet
filter(None, [x.strip() for x in re.findall(r"\b[A-Z\s]+\b", comment)])

final_tokens=[]
for tokens in result_stickers:
  for token in tokens:

    if token.lower() not in ntlk_stopwords and token.lower() not in ['ah','wsb','fomo','yolo']: