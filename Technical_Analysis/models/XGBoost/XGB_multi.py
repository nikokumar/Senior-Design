# -*- coding: utf-8 -*-
"""XGBSDTA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NGtiNp9DfUybkrSxZlRJIwWFZ3umVMRp
"""

import pandas as pd
import numpy as np
from pandas_datareader import data as pdr
import yfinance as yf
from ta.utils import dropna
import datetime as dt
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit
'''
Time Series to Supervised learning
Multi step Univariate forecasting i.e. only feature is the lagged closes of days before 
 - play with lag try one quarter of every day, next quarter every week close, every other quarter monthly closes and other options
Try Another multivariate model using rsi, ema, and macd
 - Look for other good indicators to use beyond those three such as dow, competitor companies, silicon etfs, etc.

https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/ for above

Create a function that executes buy/sell/hold commands based on xgboost confidence and predicted close * Ask dimakis could this be a full on classifier that incorporates other features like sentiment
Create an evaluator that shows the metrics of each different model
https://machinelearningmastery.com/backtest-machine-learning-models-time-series-forecasting/


'''

class XGB_multi:
    
    def __init__(self, X_lag=63):
        #Needs input of which variables so can be expanded to uni/multivariate maybe list of feature names to get from data
        # yf.pdr_override()
        # self.stock_data = pdr.get_data_yahoo("TXN") #consider saving somewhere and not downloading every time and to include which features from  list
        # self.stock_data = self.stock_data.iloc[12000:]
        # self.supervised_data = self.create_supervised_data()
        csv_path = 'Technical_Analysis/data/'
        csv_name = 'supervised_{}_days_indicators.csv'.format(X_lag)
        self.supervised_data = pd.read_csv(csv_path+csv_name)

        #read the csv
        



    def evaluate(self, n_test): #needs batch size and num features
        predictions = list()
        
        tscv = self.train_test_split(self.supervised_data, n_test)
        for train_index, test_index in tscv.split(self.supervised_data):
            train = self.supervised_data[train_index]
            test = self.supervised_data[test_index]
            pred = self.forecast(train, test)
            predictions.append(pred)
        #     X_train, X_test = X[train_index], X[test_index]
        #     y_train, y_test = y[train_index], y[test_index]
        # train, test = self.train_test_split(self.supervised_data, n_test) #can be a better call
        # history = [x for x in train]

        # for i in range(len(test)):
        #     testX, testy = test[i, :-1], test[i, -1]
        #     pred = self.forecast(history, testX)
        #     predictions.append(pred)
        #     history.append(test[i])
        
       

        return predictions, test[:, -1]

    def forecast(self, train, testX):
        train = np.asarray(train)
        trainX, trainy = train[:, :-1], train[:, -1]

        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1000) #must tune hyperparameters
        model.fit(trainX, trainy)

        #make a single prediciton
        pred = model.predict(np.asarray([testX]))

        return pred[0]


    def train_test_split(self, data, n_test):
        # data_train = data[:-n_test]
        # data_test = data[-n_test:]

        # return data_train,data_test
        tscv = TimeSeriesSplit(n_splits=n_test, test_size=1)
        return tscv #does this work for multivariate


    





